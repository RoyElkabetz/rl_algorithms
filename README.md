# rl algorithms

This repository contains Jupyter notebooks of different Reinforcement Learning (RL) algorithms implementations.

### List of algorithms:
- [REINFORCE (discrete)](notebooks/REINFORCE.ipynb)
- [REINFORCE (continuous)](notebooks/REINFORCE_continuous.ipynb)
- [Actor Critic TD(0)](notebooks/Actor_Critic_TD_0.ipynb)
- [SARSA](notebooks/SARSA.ipynb)
- [Q-Learning](notebooks/Q-Learning.ipynb)


## List of Notebooks

| Name | Description | Link | Colab | NBViewer |
|:-----|:------------|:-----|:------|:---------|
|`REINFORCE.ipynb` | implementation of the REINFORCE and REINFORCE with baseline algorithms using PyTorch and Gymnasium on the LunarLander-v2 environment | [notebook](notebooks/REINFORCE.ipynb)  | [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/REINFORCE.ipynb)        | [![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/REINFORCE.ipynb)|
|`REINFORCE_continuous.ipynb` | implementation of the REINFORCE and REINFORCE with baseline algorithms for environments with continuous action spaces using PyTorch and Gymnasium on the LunarLander-v2 continuous environment | [notebook](notebooks/REINFORCE_continuous.ipynb)  | [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/REINFORCE_continuous.ipynb)        | [![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/REINFORCE_continuous.ipynb)|
|`Actor_Critic_TD_0.ipynb` | implementation of the Actor-Critic TD(0) algorithm for environments with discrete and continuous action spaces using PyTorch and Gymnasium on the LunarLander-v2 environment | [notebook](notebooks/Actor_Critic_TD_0.ipynb)  | [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/Actor_Critic_TD_0.ipynb)        | [![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/Actor_Critic_TD_0.ipynb)|
|`SARSA.ipynb` | implementation of the SARSA algorithm with and without an Experience Replay Buffer using PyTorch and Gymnasium on the CartPole-v1 environment | [notebook](notebooks/SARSA.ipynb)  | [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/SARSA.ipynb)        | [![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/SARSA.ipynb)|
|`Q-Learning.ipynb` | implementation of the Q-Learning algorithm with and without an Experience Replay Buffer using PyTorch and Gymnasium on the Acrobot-v1 environment | [notebook](notebooks/Q-Learning.ipynb)  | [![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/Q-Learning.ipynb)        | [![nbviewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.org/github/RoyElkabetz/rl_algorithms/blob/main/notebooks/Q-Learning.ipynb)|
|   |   |   |   |   |
